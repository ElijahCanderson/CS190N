{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1262e20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import time\n",
    "import requests \n",
    "import re\n",
    "import logging\n",
    "import subprocess\n",
    "from subprocess import Popen\n",
    "from sys import platform\n",
    "import os, sys\n",
    "import logging\n",
    "import json\n",
    "import threading\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import subprocess\n",
    "import time\n",
    "import signal\n",
    "from typing import Dict\n",
    "import pandas as pd\n",
    "from netunicorn.base.architecture import Architecture\n",
    "from netunicorn.base.nodes import Node\n",
    "from netunicorn.base.task import Failure, Task, TaskDispatcher\n",
    "from netunicorn.library.tasks.upload.webdav import UploadToWebDav\n",
    "\n",
    "from netunicorn.client.remote import RemoteClient, RemoteClientException\n",
    "from netunicorn.base import Experiment, ExperimentStatus, Pipeline\n",
    "from netunicorn.library.tasks.basic import SleepTask\n",
    "from netunicorn.library.tasks.measurements.ookla_speedtest import SpeedTest\n",
    "from netunicorn.library.tasks.measurements.ping import Ping\n",
    "from netunicorn.base.architecture import Architecture\n",
    "from netunicorn.base.nodes import Node\n",
    "from netunicorn.library.tasks.capture.tcpdump import StartCapture, StopNamedCapture\n",
    "from netunicorn.base.task import Failure, Task, TaskDispatcher\n",
    "from netunicorn.base import Result, Failure, Success, Task, TaskDispatcher\n",
    "from netunicorn.base.architecture import Architecture\n",
    "from netunicorn.base.nodes import Node\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from netunicorn.library.tasks.video_watchers.youtube_watcher import WatchYouTubeVideo\n",
    "from netunicorn.library.tasks.video_watchers.vimeo_watcher import WatchVimeoVideo\n",
    "from netunicorn.library.tasks.video_watchers.twitch_watcher import WatchTwitchStream\n",
    "from typing import Dict\n",
    "from typing import Optional\n",
    "from enum import IntEnum\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from returns.pipeline import is_successful\n",
    "from returns.result import Failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bc84d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfy1 = pd.read_csv(\"/mnt/md0/cs190n/cs190n2/youtube_capture1.pcap_Flow.csv\")\n",
    "df = pd.read_csv(\"/mnt/md0/cs190n/cs190n2/twitch_capture6.pcap_Flow.csv\")\n",
    "df2 = pd.read_csv(\"/mnt/md0/cs190n/cs190n2/twitch_capture5.pcap_Flow.csv\")\n",
    "df3 = pd.read_csv(\"/mnt/md0/cs190n/cs190n2/twitch_capture4.pcap_Flow.csv\")\n",
    "df4 = pd.read_csv(\"/mnt/md0/cs190n/cs190n2/twitch_capture3.pcap_Flow.csv\")\n",
    "df5 = pd.read_csv(\"/mnt/md0/cs190n/cs190n2/twitch_capture2.pcap_Flow.csv\")\n",
    "df6 = pd.read_csv(\"/mnt/md0/cs190n/cs190n2/twitch_capture1.pcap_Flow.csv\")\n",
    "df7 = pd.read_csv(\"/mnt/md0/cs190n/cs190n2/twitch_capture7.pcap_Flow.csv\")\n",
    "df8 = pd.read_csv(\"/mnt/md0/cs190n/cs190n2/twitch_capture8.pcap_Flow.csv\")\n",
    "df9 = pd.read_csv(\"/mnt/md0/cs190n/cs190n2/twitch_capture9.pcap_Flow.csv\")\n",
    "df10 = pd.read_csv(\"/mnt/md0/cs190n/cs190n2/twitch_capture10.pcap_Flow.csv\")\n",
    "df11 = pd.read_csv(\"/mnt/md0/cs190n/cs190n2/twitch_capture11.pcap_Flow.csv\")\n",
    "Twtichcongest = pd.read_csv(\"/mnt/md0/cs190n/cs190n2/twitchCongest_capture4.pcap_Flow.csv\")\n",
    "TwitchCongest2 = pd.read_csv(\"/mnt/md0/cs190n/cs190n2/twitchCongest_capture5.pcap_Flow.csv\")\n",
    "TwitchCongestlarge = pd.read_csv(\"/mnt/md0/cs190n/cs190n2/twitchCongest_capture6.pcap_Flow.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcb73c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([dfy1,df, df2, df3, df4, df5, df6, df7, df8, df9, df10,df11, Twtichcongest, TwitchCongest2, TwitchCongestlarge ], ignore_index=True)\n",
    "\n",
    "#merged_df = TwitchCongest2\n",
    "merged_df.drop_duplicates(inplace=True)\n",
    "#print(merged_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eac705bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df is the DataFrame containing the flow data\n",
    "\n",
    "# Sort the DataFrame by Timestamp (earliest first)\n",
    "df_sorted = merged_df.sort_values(by='Timestamp', ascending=True)\n",
    "#print(df_sorted.shape)\n",
    "df_sorted = df_sorted[df_sorted['Flow Bytes/s'] > 0]\n",
    "# Convert Flow Duration to seconds\n",
    "df_sorted['Flow Duration'] = df_sorted['Flow Duration'] / 1000000 \n",
    "\n",
    "# Estimate RTT using Flow IAT Mean (forward or backward)\n",
    "# We assume RTT as twice the average Flow IAT Mean (forward and backward directions)\n",
    "df_sorted['Estimated RTT (Seconds)'] = df_sorted['Flow IAT Mean'] * 2 / 1000000\n",
    "\n",
    "# Estimate the available bandwidth from Flow Bytes/s (in bytes per second)\n",
    "df_sorted['Bandwidth (Bytes/s)'] = df_sorted['Flow Bytes/s']  # Assuming Flow Bytes/s gives available bandwidth\n",
    "\n",
    "# Calculate the Bandwidth-Delay Product (BDP) to estimate CWND (in bytes)\n",
    "df_sorted['Estimated CWND (Bytes)'] = df_sorted['Bandwidth (Bytes/s)'] * df_sorted['Estimated RTT (Seconds)']\n",
    "\n",
    "# Calculate the total flow size (in bytes)\n",
    "df_sorted['Total Flow Size (Bytes)'] = df_sorted['Flow Bytes/s'] * df_sorted['Flow Duration']\n",
    "#print(df_sorted.shape)\n",
    "\n",
    "\n",
    "# Create a new DataFrame with the required columns\n",
    "flow_size_df = pd.DataFrame({\n",
    "    'Flow ID': df_sorted['Flow ID'],\n",
    "    'cwnd': df_sorted['Estimated CWND (Bytes)'],\n",
    "    #'Flow Bytes/s': df_sorted['Flow Bytes/s'],\n",
    "    'rtt': df_sorted['Estimated RTT (Seconds)'],\n",
    "    'Timestamp': df_sorted['Timestamp'],\n",
    "    'size': df_sorted['Total Flow Size (Bytes)'],\n",
    "    'Duration': df_sorted['Flow Duration'], # Add Total Flow Size column\n",
    "    #'Fwd IAT Mean' : df_sorted['Fwd IAT Mean'],\n",
    "    #'Fwd Packet/Bulk Avg' : df_sorted['Fwd Packet/Bulk Avg'],\n",
    "    'Average Packet Size' : df_sorted['Average Packet Size'],\n",
    "    'Flow Packets/s' : df_sorted['Flow Packets/s'],\n",
    "    #'Bwd IAT Mean' : df_sorted['Bwd IAT Mean'],\n",
    "    #'Flow IAT Mean' : df_sorted['Flow IAT Mean'],\n",
    "    \n",
    "    \n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "# Display the result\n",
    "#print(flow_size_df.shape)\n",
    "#print(df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a55d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = flow_size_df[['size','rtt', 'cwnd', 'Average Packet Size']]\n",
    "y = flow_size_df['Duration']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b419f074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in the name of your working node\n",
    "working_node = 'aws-fargate-B-cs190n2-'\n",
    "#data = pd.read_csv('Data/ssim_2019-01-26T11_2019-01-27T11.csv') \n",
    "#video_sent = pd.read_csv('Data/video_sent_2024-11-15T11_2024-11-16T11.csv')\n",
    "#video_acked = pd.read_csv('Data/video_acked_2024-11-15T11_2024-11-16T11.csv')\n",
    "#print(video_sent.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c47d0fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUFFER DATA\n",
    "# Merge on common columns to align each chunk sent with its ack\n",
    "\n",
    "#merged_data = pd.merge(\n",
    " #   video_sent, \n",
    " #   video_acked, \n",
    " #   on=['session_id', 'index', 'video_ts'], \n",
    " #   suffixes=('_sent', '_acked')\n",
    "#)\n",
    "\n",
    "# Exclude rows where delivery rate is zero\n",
    "#merged_data = merged_data[merged_data['delivery_rate'] != 0]\n",
    "\n",
    "# Display first few rows of the merged data to verify\n",
    "#print(merged_data[merged_data['delivery_rate'] == 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "050d9c60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PUFFER DATA\n",
    "#merged_data.rename(columns={'time (ns GMT)_sent': 'time_sent', 'time (ns GMT)_acked': 'time_acked'}, inplace=True)\n",
    "\n",
    "#merged_data['download_time'] = (merged_data['size'] / merged_data['delivery_rate'])\n",
    "#print(merged_data[['session_id', 'index', 'download_time', 'size']])\n",
    "# Filter rows where session_id is 'Hi'\n",
    "#filtered_data = merged_data[merged_data['session_id'] == \"kaXlFjXCNjAgx4nsvYsDetENDzv04n/e7R4HYutFWt0=\"]\n",
    "\n",
    "# Print the filtered rows\n",
    "#print(filtered_data[['session_id', 'index', 'download_time']])\n",
    "#\n",
    "\n",
    "#X = merged_data[['size','rtt', 'cwnd']]\n",
    "#y = merged_data['download_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15f1ec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11352940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362ddba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "model = SGDRegressor(max_iter=10000, tol=1e-3, learning_rate='optimal', loss='huber', epsilon=1.35)\n",
    "#model = SGDRegressor(max_iter=10000, tol=1e-3,learning_rate='optimal')\n",
    "#model.fit(X_train,y_train)\n",
    "#model = SGDRegressor(max_iter=10000, tol=1e-3)\n",
    "decay_factor = 1\n",
    "train_size = 20\n",
    "test_size = 2\n",
    "# Store results\n",
    "mse_list = []\n",
    "mae_list = []\n",
    "\n",
    "# Start incremental training\n",
    "for start in range(0, len(X_scaled) - train_size - test_size, test_size):\n",
    "    train_end = start + train_size\n",
    "    test_end = train_end + test_size\n",
    "\n",
    "    # Split data incrementally\n",
    "    X_train = X_scaled[start:train_end]\n",
    "    y_train = y.iloc[start:train_end]\n",
    "    X_test = X_scaled[train_end:test_end]\n",
    "    y_test = y.iloc[train_end:test_end]\n",
    "    \n",
    "    # Train the model incrementally with weights\n",
    "    #print(weights)\n",
    "    # Perform partial fitting\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    #print(y_test)\n",
    "    # If MSE is too high, reset the model\n",
    "    if(mse > 500):\n",
    "        model = SGDRegressor(max_iter=10000, tol=1e-3, loss='huber', epsilon=1.35) \n",
    "    # Append MSE and MAE to their respective lists\n",
    "    mse_list.append(mse)\n",
    "    mae_list.append(mae)\n",
    "    print(y_pred, y_test)\n",
    "    # Print results for the current step\n",
    "    #print(mse, mae)\n",
    "\n",
    "# Print overall results\n",
    "print(\"Mean Squared Error across all steps:\", mse_list)\n",
    "print(\"Mean Absolute Error across all steps:\", mae_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7d5bb5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(p, g, test_size=0.2, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2f5bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515a4796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "mse_list = [mse for i, mse in enumerate(mse_list) if mse < 400 and i % 5 == 0]\n",
    "mae_list = [mae for i, mae in enumerate(mae_list) if mae < 400 and i % 5 == 0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(mae_list, label=\"MAE\")\n",
    "plt.plot(mse_list, label=\"MSE\")\n",
    "\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Error')\n",
    "plt.title('MSE + MAE (Twitch + Youtube)')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d51b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac53697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a827085b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
